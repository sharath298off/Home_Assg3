{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharath298off/Home_Assg3/blob/main/NeuralNwAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M85DDWRL8DpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Implementing an RNN for Text Generation"
      ],
      "metadata": {
        "id": "9BYO9MR1Mtlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7WiS-EHMoB-",
        "outputId": "906ec8cc-b5c8-47f0-fa8a-dd962d3bde02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - loss: 3.1772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e4c23d295d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# ✅ Step 1: Import Libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# ✅ Step 2: Load Shakespeare Text\n",
        "path = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
        "        \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# ✅ Step 3: Preprocess Text\n",
        "vocab = sorted(set(text))\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "seq_length = 100\n",
        "sequences = tf.data.Dataset.from_tensor_slices(all_ids).batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# ✅ Step 4: Batch and Shuffle\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ✅ Step 5: Define LSTM Model (Subclassed)\n",
        "class TextGenModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        if states is None:\n",
        "            x, state_h, state_c = self.lstm(x, training=training)\n",
        "        else:\n",
        "            x, state_h, state_c = self.lstm(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, [state_h, state_c]\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "# ✅ Step 6: Build & Compile Model\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = TextGenModel(vocab_size, embedding_dim, rnn_units)\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# ✅ Step 7: Train (small epoch for testing)\n",
        "EPOCHS = 1  # Set to 10 or 20 for better output later\n",
        "model.fit(dataset, epochs=EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "        self.temperature = temperature\n",
        "\n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            indices=skip_ids,\n",
        "            values=[-float('inf')]*len(skip_ids),\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "        predicted_logits = predicted_logits[:, -1, :] / self.temperature\n",
        "        predicted_logits += self.prediction_mask\n",
        "\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        return predicted_chars, states\n"
      ],
      "metadata": {
        "id": "62WoRUxwQTKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instance of OneStep\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant([\"Once upon a time,\"])\n",
        "result = [next_char]\n",
        "\n",
        "for _ in range(500):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "generated_text = tf.strings.join(result)\n",
        "print(\"\\n--- Generated Text ---\\n\")\n",
        "print(generated_text[0].numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia6_B30iQZN_",
        "outputId": "498cac84-b63e-4591-80e3-92597b94a3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Text ---\n",
            "\n",
            "Once upon a time,, thel, il he is brolk Garan, ho hose tor hon han;\n",
            "Her fare lo kiens! in fing hisn ow mor heabe yaroud chyiciges.\n",
            "kt hath heees an hals, hnsws there An swith end doreinge.\n",
            "Whicl y' foray this heet hor am;\n",
            "'ow all il mod?\n",
            "re hourn:\n",
            "Se siste fart cisfer; I ne theen,\n",
            "An whay I the arpeshea.\n",
            "Now thee I berapw woone wiles e bebes fearlmide ive in dant tpase kejond and bo an tame\n",
            "And math conde:, tho, you! ascow, hay thabl gode\n",
            "Mat Gorey, Dlall thaeng bane hent oselly, hat siste shelu,,\n",
            "Siny have fart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: NLP Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "NZ09UdNlQwO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "# Download stopwords only\n",
        "nltk.download('stopwords', force=True)\n",
        "\n",
        "# ✅ Step 1: Input sentence\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "\n",
        "# ✅ Step 2: Custom Tokenizer (no punkt)\n",
        "tokens = re.findall(r'\\b\\w+\\b', sentence)\n",
        "\n",
        "# ✅ Step 3: Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# ✅ Step 4: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# ✅ Output\n",
        "print(\"1. Original Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n2. Tokens Without Stopwords:\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\n3. Stemmed Words:\")\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5XwLVnZQw58",
        "outputId": "b60baf77-59b1-4e0c-efa9-5bc9f2388cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Original Tokens:\n",
            "['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri']\n",
            "\n",
            "2. Tokens Without Stopwords:\n",
            "['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri']\n",
            "\n",
            "3. Stemmed Words:\n",
            "['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the difference between stemming and lemmatization? Provide examples with the word “running.”"
      ],
      "metadata": {
        "id": "dnS9tRDISknl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a rule-based process that chops off word endings to reduce a word to its root form, often without considering whether the result is a valid word.\n",
        "\n",
        "Lemmatization, on the other hand, uses a dictionary and grammar rules to convert a word to its base or lemma form, which is always a valid word.\n",
        "\n",
        "Feature\tStemming\tLemmatization\n",
        "Logic\tRule-based cutting\tDictionary + grammar-based\n",
        "Accuracy\tLess accurate\tMore accurate\n",
        "Example (\"running\")\t→ run (or even runn)\t→ run\n",
        "Output Valid Word?\tNot always\tYes\n",
        "\n",
        "📝 Example:\n",
        "\n",
        "\"running\" → Stemming: runn\n",
        "\n",
        "\"running\" → Lemmatization: run"
      ],
      "metadata": {
        "id": "pF7T8mF3Slxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Why might removing stop words be useful in some NLP tasks, and when might it actually be harmful?"
      ],
      "metadata": {
        "id": "64DjnDdLSv8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful When:\n",
        "Stopwords (like \"the\", \"is\", \"in\") don’t add meaningful value to the task.\n",
        "\n",
        "Helps in reducing dimensionality and noise.\n",
        "\n",
        "Improves performance in:\n",
        "\n",
        "Text classification\n",
        "\n",
        "Topic modeling\n",
        "\n",
        "Clustering\n",
        "\n",
        "🔹 Example: In sentiment analysis, “not good” becomes “good” if “not” is removed — which leads to incorrect sentiment.\n",
        "\n",
        "❌ Harmful When:\n",
        "Stopwords may carry important meaning in context.\n",
        "\n",
        "In tasks like:\n",
        "\n",
        "Machine Translation\n",
        "\n",
        "Question Answering\n",
        "\n",
        "Text Summarization\n",
        "\n",
        "Removing stopwords like “not,” “never,” or “until” may reverse the meaning of the sentence"
      ],
      "metadata": {
        "id": "EnSRCAX8S1RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Named Entity Recognition with spaCy"
      ],
      "metadata": {
        "id": "0vObj6nuT4E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 1: Install spaCy and English model (uncomment if using in Colab)\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# ✅ Step 2: Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ✅ Step 3: Input sentence\n",
        "text = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "\n",
        "# ✅ Step 4: Apply spaCy NER\n",
        "doc = nlp(text)\n",
        "\n",
        "# ✅ Step 5: Print all named entities\n",
        "print(\"Named Entities Found:\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}\")\n",
        "    print(f\"Label: {ent.label_}\")\n",
        "    print(f\"Start Char: {ent.start_char}, End Char: {ent.end_char}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFZ7aBNeS9K7",
        "outputId": "1e014320-96f4-41dc-bf95-3eda0d3047d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities Found:\n",
            "\n",
            "Entity: Barack Obama\n",
            "Label: PERSON\n",
            "Start Char: 0, End Char: 12\n",
            "---\n",
            "Entity: 44th\n",
            "Label: ORDINAL\n",
            "Start Char: 27, End Char: 31\n",
            "---\n",
            "Entity: the United States\n",
            "Label: GPE\n",
            "Start Char: 45, End Char: 62\n",
            "---\n",
            "Entity: the Nobel Peace Prize\n",
            "Label: WORK_OF_ART\n",
            "Start Char: 71, End Char: 92\n",
            "---\n",
            "Entity: 2009\n",
            "Label: DATE\n",
            "Start Char: 96, End Char: 100\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How does NER differ from POS tagging in NLP?"
      ],
      "metadata": {
        "id": "JzeLoaKIUOCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference Between NER and POS Tagging\n",
        "Feature\tNER (Named Entity Recognition)\tPOS Tagging (Part-of-Speech Tagging)\n",
        "Purpose\tIdentifies real-world entities like names, dates\tIdentifies grammatical role of each word\n",
        "Output\tEntity label (e.g., PERSON, ORG, DATE)\tPOS tag (e.g., Noun, Verb, Adjective)\n",
        "Example\t\"Barack Obama\" → PERSON\t\"Obama\" → Proper Noun (NNP)\n",
        "Use Case\tUsed in Information Extraction, Search Engines\tUsed in Syntax Analysis, Parsing\n",
        "\n",
        "🧠 NER helps machines understand who/what/where in a sentence.\n",
        "🧠 POS helps machines understand the role of each word in grammar."
      ],
      "metadata": {
        "id": "71PUOUtNUO_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Describe two applications that use NER in the real world."
      ],
      "metadata": {
        "id": "DtF_b8qGU1Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Financial News Analysis\n",
        "NER helps extract company names, stock tickers, and monetary values from articles.\n",
        "\n",
        "Example:\n",
        "“Apple acquired Beats for $3 billion”\n",
        "→ Entities: Apple (ORG), $3 billion (MONEY)\n",
        "\n",
        "✅ 2. Search Engines / Virtual Assistants\n",
        "Improves query understanding by detecting entities in user questions.\n",
        "\n",
        "Example:\n",
        "“When did Nelson Mandela become president?”\n",
        "→ Entities: Nelson Mandela (PERSON), president (TITLE)"
      ],
      "metadata": {
        "id": "cEKNIKvuUoZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "Y29Wm3JcVm2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ✅ Step 1: Define Q, K, V\n",
        "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "\n",
        "# ✅ Step 2: Scaled Dot-Product Attention Function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]  # key dimension (usually d_model)\n",
        "\n",
        "    # Step 1: Compute raw attention scores → Q @ K^T\n",
        "    scores = np.matmul(Q, K.T)\n",
        "\n",
        "    # Step 2: Scale by sqrt(d)\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "    # Step 3: Apply softmax to get attention weights\n",
        "    attention_weights = tf.nn.softmax(scaled_scores, axis=-1).numpy()\n",
        "\n",
        "    # Step 4: Multiply attention weights by V\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return attention_weights, output\n",
        "\n",
        "# ✅ Step 3: Run attention\n",
        "attention_weights, output = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# ✅ Step 4: Display results\n",
        "print(\"Attention Weights (Softmax Output):\")\n",
        "print(np.round(attention_weights, 4))\n",
        "\n",
        "print(\"\\nFinal Output (Attention × V):\")\n",
        "print(np.round(output, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i_-MAr6VnkI",
        "outputId": "207d64ac-9d71-48e4-b095-758833f426bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights (Softmax Output):\n",
            "[[0.7311 0.2689]\n",
            " [0.2689 0.7311]]\n",
            "\n",
            "Final Output (Attention × V):\n",
            "[[2.0758 3.0758 4.0758 5.0758]\n",
            " [3.9242 4.9242 5.9242 6.9242]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why do we divide the attention score by √d in the scaled dot-product attention formula?"
      ],
      "metadata": {
        "id": "VS5iCElAVs7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the dot product of Q and K becomes large (especially with high-dimensional vectors), the softmax function can produce extremely small gradients, making learning difficult.\n",
        "\n",
        "✅ Dividing by √d (where d is the key dimension) scales down large values, stabilizing gradients and improving training convergence."
      ],
      "metadata": {
        "id": "eGPxL8mHV0bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does self-attention help the model understand relationships between words in a sentence?"
      ],
      "metadata": {
        "id": "OCW_pk-iV34_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention lets each word attend to (or look at) every other word in the sequence, regardless of position.\n",
        "\n",
        "✅ This helps the model:\n",
        "\n",
        "Capture contextual meaning (e.g., \"bank\" in \"river bank\" vs \"money bank\")\n",
        "\n",
        "Understand long-range dependencies\n",
        "\n",
        "Process words in parallel, enabling transformer models like BERT and GPT\n",
        "\n",
        "Example:\n",
        "In the sentence “The dog that chased the cat barked,”\n",
        "→ self-attention allows “barked” to focus on “dog” instead of “cat”."
      ],
      "metadata": {
        "id": "DpLaxn0oV7S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Sentiment Analysis with HuggingFace Transformers"
      ],
      "metadata": {
        "id": "jjwh1IplV_vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 1: Install HuggingFace Transformers if needed\n",
        "# Uncomment below line if using Google Colab\n",
        "# !pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# ✅ Step 2: Load pre-trained sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# ✅ Step 3: Input sentence\n",
        "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
        "\n",
        "# ✅ Step 4: Run sentiment analysis\n",
        "result = classifier(sentence)[0]\n",
        "\n",
        "# ✅ Step 5: Print output\n",
        "print(\"Sentiment:\", result['label'])\n",
        "print(\"Confidence Score:\", round(result['score'], 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iJL3hnVWQhD",
        "outputId": "3c114667-6900-419b-ac32-57c87f38a44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?"
      ],
      "metadata": {
        "id": "7Ybw2rHWW-Ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main architectural difference lies in how they use the Transformer architecture:\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers)\n",
        "→ Uses only the encoder part of the Transformer\n",
        "→ Processes text bidirectionally (looks both left and right)\n",
        "\n",
        "GPT (Generative Pre-trained Transformer)\n",
        "→ Uses only the decoder part of the Transformer\n",
        "→ Processes text unidirectionally (left-to-right)\n",
        "\n",
        "Summary:\n",
        "\n",
        "BERT = Encoder (good for understanding tasks like classification, QA)\n",
        "\n",
        "GPT = Decoder (good for generation tasks like text completion, summarization)"
      ],
      "metadata": {
        "id": "UWncewyWW_4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Why is using pre-trained models (like BERT or GPT) beneficial for NLP applications instead of training from scratch?"
      ],
      "metadata": {
        "id": "3rt6CMxOXI1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pre-trained models is beneficial because:\n",
        "\n",
        "✅ They are trained on massive datasets (e.g., Wikipedia, BooksCorpus), which gives them a strong understanding of language.\n",
        "\n",
        "✅ Saves time and computational cost — training from scratch requires millions of samples and powerful GPUs.\n",
        "\n",
        "✅ Can be fine-tuned on small datasets for specific tasks like sentiment analysis, NER, or summarization.\n",
        "\n",
        "✅ Often achieve state-of-the-art accuracy in NLP tasks even with minimal labeled data.\n",
        "\n",
        "📌 In short, pre-trained models let you stand on the shoulders of giants — reusing powerful language understanding without starting from zero."
      ],
      "metadata": {
        "id": "g3tNOxKDXLsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DH5niLkfXT2W"
      }
    }
  ]
}