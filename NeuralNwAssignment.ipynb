{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharath298off/Home_Assg3/blob/main/NeuralNwAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M85DDWRL8DpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Implementing an RNN for Text Generation"
      ],
      "metadata": {
        "id": "9BYO9MR1Mtlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7WiS-EHMoB-",
        "outputId": "906ec8cc-b5c8-47f0-fa8a-dd962d3bde02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m172/172\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - loss: 3.1772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e4c23d295d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# ‚úÖ Step 1: Import Libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# ‚úÖ Step 2: Load Shakespeare Text\n",
        "path = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
        "        \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# ‚úÖ Step 3: Preprocess Text\n",
        "vocab = sorted(set(text))\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "seq_length = 100\n",
        "sequences = tf.data.Dataset.from_tensor_slices(all_ids).batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# ‚úÖ Step 4: Batch and Shuffle\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ‚úÖ Step 5: Define LSTM Model (Subclassed)\n",
        "class TextGenModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        if states is None:\n",
        "            x, state_h, state_c = self.lstm(x, training=training)\n",
        "        else:\n",
        "            x, state_h, state_c = self.lstm(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, [state_h, state_c]\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "# ‚úÖ Step 6: Build & Compile Model\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = TextGenModel(vocab_size, embedding_dim, rnn_units)\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# ‚úÖ Step 7: Train (small epoch for testing)\n",
        "EPOCHS = 1  # Set to 10 or 20 for better output later\n",
        "model.fit(dataset, epochs=EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "        self.temperature = temperature\n",
        "\n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            indices=skip_ids,\n",
        "            values=[-float('inf')]*len(skip_ids),\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "        predicted_logits = predicted_logits[:, -1, :] / self.temperature\n",
        "        predicted_logits += self.prediction_mask\n",
        "\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        return predicted_chars, states\n"
      ],
      "metadata": {
        "id": "62WoRUxwQTKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instance of OneStep\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant([\"Once upon a time,\"])\n",
        "result = [next_char]\n",
        "\n",
        "for _ in range(500):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "generated_text = tf.strings.join(result)\n",
        "print(\"\\n--- Generated Text ---\\n\")\n",
        "print(generated_text[0].numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia6_B30iQZN_",
        "outputId": "498cac84-b63e-4591-80e3-92597b94a3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Text ---\n",
            "\n",
            "Once upon a time,, thel, il he is brolk Garan, ho hose tor hon han;\n",
            "Her fare lo kiens! in fing hisn ow mor heabe yaroud chyiciges.\n",
            "kt hath heees an hals, hnsws there An swith end doreinge.\n",
            "Whicl y' foray this heet hor am;\n",
            "'ow all il mod?\n",
            "re hourn:\n",
            "Se siste fart cisfer; I ne theen,\n",
            "An whay I the arpeshea.\n",
            "Now thee I berapw woone wiles e bebes fearlmide ive in dant tpase kejond and bo an tame\n",
            "And math conde:, tho, you! ascow, hay thabl gode\n",
            "Mat Gorey, Dlall thaeng bane hent oselly, hat siste shelu,,\n",
            "Siny have fart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: NLP Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "NZ09UdNlQwO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "\n",
        "# Download stopwords only\n",
        "nltk.download('stopwords', force=True)\n",
        "\n",
        "# ‚úÖ Step 1: Input sentence\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "\n",
        "# ‚úÖ Step 2: Custom Tokenizer (no punkt)\n",
        "tokens = re.findall(r'\\b\\w+\\b', sentence)\n",
        "\n",
        "# ‚úÖ Step 3: Stopword removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# ‚úÖ Step 4: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# ‚úÖ Output\n",
        "print(\"1. Original Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n2. Tokens Without Stopwords:\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\n3. Stemmed Words:\")\n",
        "print(stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5XwLVnZQw58",
        "outputId": "b60baf77-59b1-4e0c-efa9-5bc9f2388cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Original Tokens:\n",
            "['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri']\n",
            "\n",
            "2. Tokens Without Stopwords:\n",
            "['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri']\n",
            "\n",
            "3. Stemmed Words:\n",
            "['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the difference between stemming and lemmatization? Provide examples with the word ‚Äúrunning.‚Äù"
      ],
      "metadata": {
        "id": "dnS9tRDISknl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is a rule-based process that chops off word endings to reduce a word to its root form, often without considering whether the result is a valid word.\n",
        "\n",
        "Lemmatization, on the other hand, uses a dictionary and grammar rules to convert a word to its base or lemma form, which is always a valid word.\n",
        "\n",
        "Feature\tStemming\tLemmatization\n",
        "Logic\tRule-based cutting\tDictionary + grammar-based\n",
        "Accuracy\tLess accurate\tMore accurate\n",
        "Example (\"running\")\t‚Üí run (or even runn)\t‚Üí run\n",
        "Output Valid Word?\tNot always\tYes\n",
        "\n",
        "üìù Example:\n",
        "\n",
        "\"running\" ‚Üí Stemming: runn\n",
        "\n",
        "\"running\" ‚Üí Lemmatization: run"
      ],
      "metadata": {
        "id": "pF7T8mF3Slxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Why might removing stop words be useful in some NLP tasks, and when might it actually be harmful?"
      ],
      "metadata": {
        "id": "64DjnDdLSv8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful When:\n",
        "Stopwords (like \"the\", \"is\", \"in\") don‚Äôt add meaningful value to the task.\n",
        "\n",
        "Helps in reducing dimensionality and noise.\n",
        "\n",
        "Improves performance in:\n",
        "\n",
        "Text classification\n",
        "\n",
        "Topic modeling\n",
        "\n",
        "Clustering\n",
        "\n",
        "üîπ Example: In sentiment analysis, ‚Äúnot good‚Äù becomes ‚Äúgood‚Äù if ‚Äúnot‚Äù is removed ‚Äî which leads to incorrect sentiment.\n",
        "\n",
        "‚ùå Harmful When:\n",
        "Stopwords may carry important meaning in context.\n",
        "\n",
        "In tasks like:\n",
        "\n",
        "Machine Translation\n",
        "\n",
        "Question Answering\n",
        "\n",
        "Text Summarization\n",
        "\n",
        "Removing stopwords like ‚Äúnot,‚Äù ‚Äúnever,‚Äù or ‚Äúuntil‚Äù may reverse the meaning of the sentence"
      ],
      "metadata": {
        "id": "EnSRCAX8S1RV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Named Entity Recognition with spaCy"
      ],
      "metadata": {
        "id": "0vObj6nuT4E5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install spaCy and English model (uncomment if using in Colab)\n",
        "# !pip install -U spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# ‚úÖ Step 2: Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ‚úÖ Step 3: Input sentence\n",
        "text = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "\n",
        "# ‚úÖ Step 4: Apply spaCy NER\n",
        "doc = nlp(text)\n",
        "\n",
        "# ‚úÖ Step 5: Print all named entities\n",
        "print(\"Named Entities Found:\\n\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}\")\n",
        "    print(f\"Label: {ent.label_}\")\n",
        "    print(f\"Start Char: {ent.start_char}, End Char: {ent.end_char}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFZ7aBNeS9K7",
        "outputId": "1e014320-96f4-41dc-bf95-3eda0d3047d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities Found:\n",
            "\n",
            "Entity: Barack Obama\n",
            "Label: PERSON\n",
            "Start Char: 0, End Char: 12\n",
            "---\n",
            "Entity: 44th\n",
            "Label: ORDINAL\n",
            "Start Char: 27, End Char: 31\n",
            "---\n",
            "Entity: the United States\n",
            "Label: GPE\n",
            "Start Char: 45, End Char: 62\n",
            "---\n",
            "Entity: the Nobel Peace Prize\n",
            "Label: WORK_OF_ART\n",
            "Start Char: 71, End Char: 92\n",
            "---\n",
            "Entity: 2009\n",
            "Label: DATE\n",
            "Start Char: 96, End Char: 100\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How does NER differ from POS tagging in NLP?"
      ],
      "metadata": {
        "id": "JzeLoaKIUOCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference Between NER and POS Tagging\n",
        "Feature\tNER (Named Entity Recognition)\tPOS Tagging (Part-of-Speech Tagging)\n",
        "Purpose\tIdentifies real-world entities like names, dates\tIdentifies grammatical role of each word\n",
        "Output\tEntity label (e.g., PERSON, ORG, DATE)\tPOS tag (e.g., Noun, Verb, Adjective)\n",
        "Example\t\"Barack Obama\" ‚Üí PERSON\t\"Obama\" ‚Üí Proper Noun (NNP)\n",
        "Use Case\tUsed in Information Extraction, Search Engines\tUsed in Syntax Analysis, Parsing\n",
        "\n",
        "üß† NER helps machines understand who/what/where in a sentence.\n",
        "üß† POS helps machines understand the role of each word in grammar."
      ],
      "metadata": {
        "id": "71PUOUtNUO_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Describe two applications that use NER in the real world."
      ],
      "metadata": {
        "id": "DtF_b8qGU1Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Financial News Analysis\n",
        "NER helps extract company names, stock tickers, and monetary values from articles.\n",
        "\n",
        "Example:\n",
        "‚ÄúApple acquired Beats for $3 billion‚Äù\n",
        "‚Üí Entities: Apple (ORG), $3 billion (MONEY)\n",
        "\n",
        "‚úÖ 2. Search Engines / Virtual Assistants\n",
        "Improves query understanding by detecting entities in user questions.\n",
        "\n",
        "Example:\n",
        "‚ÄúWhen did Nelson Mandela become president?‚Äù\n",
        "‚Üí Entities: Nelson Mandela (PERSON), president (TITLE)"
      ],
      "metadata": {
        "id": "cEKNIKvuUoZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "Y29Wm3JcVm2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ‚úÖ Step 1: Define Q, K, V\n",
        "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "\n",
        "# ‚úÖ Step 2: Scaled Dot-Product Attention Function\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]  # key dimension (usually d_model)\n",
        "\n",
        "    # Step 1: Compute raw attention scores ‚Üí Q @ K^T\n",
        "    scores = np.matmul(Q, K.T)\n",
        "\n",
        "    # Step 2: Scale by sqrt(d)\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "    # Step 3: Apply softmax to get attention weights\n",
        "    attention_weights = tf.nn.softmax(scaled_scores, axis=-1).numpy()\n",
        "\n",
        "    # Step 4: Multiply attention weights by V\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return attention_weights, output\n",
        "\n",
        "# ‚úÖ Step 3: Run attention\n",
        "attention_weights, output = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# ‚úÖ Step 4: Display results\n",
        "print(\"Attention Weights (Softmax Output):\")\n",
        "print(np.round(attention_weights, 4))\n",
        "\n",
        "print(\"\\nFinal Output (Attention √ó V):\")\n",
        "print(np.round(output, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i_-MAr6VnkI",
        "outputId": "207d64ac-9d71-48e4-b095-758833f426bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights (Softmax Output):\n",
            "[[0.7311 0.2689]\n",
            " [0.2689 0.7311]]\n",
            "\n",
            "Final Output (Attention √ó V):\n",
            "[[2.0758 3.0758 4.0758 5.0758]\n",
            " [3.9242 4.9242 5.9242 6.9242]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why do we divide the attention score by ‚àöd in the scaled dot-product attention formula?"
      ],
      "metadata": {
        "id": "VS5iCElAVs7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the dot product of Q and K becomes large (especially with high-dimensional vectors), the softmax function can produce extremely small gradients, making learning difficult.\n",
        "\n",
        "‚úÖ Dividing by ‚àöd (where d is the key dimension) scales down large values, stabilizing gradients and improving training convergence."
      ],
      "metadata": {
        "id": "eGPxL8mHV0bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does self-attention help the model understand relationships between words in a sentence?"
      ],
      "metadata": {
        "id": "OCW_pk-iV34_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-attention lets each word attend to (or look at) every other word in the sequence, regardless of position.\n",
        "\n",
        "‚úÖ This helps the model:\n",
        "\n",
        "Capture contextual meaning (e.g., \"bank\" in \"river bank\" vs \"money bank\")\n",
        "\n",
        "Understand long-range dependencies\n",
        "\n",
        "Process words in parallel, enabling transformer models like BERT and GPT\n",
        "\n",
        "Example:\n",
        "In the sentence ‚ÄúThe dog that chased the cat barked,‚Äù\n",
        "‚Üí self-attention allows ‚Äúbarked‚Äù to focus on ‚Äúdog‚Äù instead of ‚Äúcat‚Äù."
      ],
      "metadata": {
        "id": "DpLaxn0oV7S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Sentiment Analysis with HuggingFace Transformers"
      ],
      "metadata": {
        "id": "jjwh1IplV_vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install HuggingFace Transformers if needed\n",
        "# Uncomment below line if using Google Colab\n",
        "# !pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# ‚úÖ Step 2: Load pre-trained sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# ‚úÖ Step 3: Input sentence\n",
        "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
        "\n",
        "# ‚úÖ Step 4: Run sentiment analysis\n",
        "result = classifier(sentence)[0]\n",
        "\n",
        "# ‚úÖ Step 5: Print output\n",
        "print(\"Sentiment:\", result['label'])\n",
        "print(\"Confidence Score:\", round(result['score'], 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iJL3hnVWQhD",
        "outputId": "3c114667-6900-419b-ac32-57c87f38a44d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?"
      ],
      "metadata": {
        "id": "7Ybw2rHWW-Ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main architectural difference lies in how they use the Transformer architecture:\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers)\n",
        "‚Üí Uses only the encoder part of the Transformer\n",
        "‚Üí Processes text bidirectionally (looks both left and right)\n",
        "\n",
        "GPT (Generative Pre-trained Transformer)\n",
        "‚Üí Uses only the decoder part of the Transformer\n",
        "‚Üí Processes text unidirectionally (left-to-right)\n",
        "\n",
        "Summary:\n",
        "\n",
        "BERT = Encoder (good for understanding tasks like classification, QA)\n",
        "\n",
        "GPT = Decoder (good for generation tasks like text completion, summarization)"
      ],
      "metadata": {
        "id": "UWncewyWW_4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Why is using pre-trained models (like BERT or GPT) beneficial for NLP applications instead of training from scratch?"
      ],
      "metadata": {
        "id": "3rt6CMxOXI1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pre-trained models is beneficial because:\n",
        "\n",
        "‚úÖ They are trained on massive datasets (e.g., Wikipedia, BooksCorpus), which gives them a strong understanding of language.\n",
        "\n",
        "‚úÖ Saves time and computational cost ‚Äî training from scratch requires millions of samples and powerful GPUs.\n",
        "\n",
        "‚úÖ Can be fine-tuned on small datasets for specific tasks like sentiment analysis, NER, or summarization.\n",
        "\n",
        "‚úÖ Often achieve state-of-the-art accuracy in NLP tasks even with minimal labeled data.\n",
        "\n",
        "üìå In short, pre-trained models let you stand on the shoulders of giants ‚Äî reusing powerful language understanding without starting from zero."
      ],
      "metadata": {
        "id": "g3tNOxKDXLsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DH5niLkfXT2W"
      }
    }
  ]
}